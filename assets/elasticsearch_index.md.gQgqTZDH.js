import{_ as e,c as a,o as t,a4 as i}from"./chunks/framework.PLIGnzYk.js";const m=JSON.parse('{"title":"Elasticsearch学习笔记","description":"","frontmatter":{},"headers":[],"relativePath":"elasticsearch/index.md","filePath":"elasticsearch/index.md","lastUpdated":1727780746000}'),s={name:"elasticsearch/index.md"},l=i(`<h1 id="elasticsearch学习笔记" tabindex="-1">Elasticsearch学习笔记 <a class="header-anchor" href="#elasticsearch学习笔记" aria-label="Permalink to &quot;Elasticsearch学习笔记&quot;">​</a></h1><nav class="table-of-contents"><ul><li><a href="#es语法">ES语法</a><ul><li><a href="#查询">查询</a></li></ul></li><li><a href="#es写数据、读数据过程">ES写数据、读数据过程</a></li><li><a href="#references">References</a></li></ul></nav><blockquote><p>官方文档：<a href="https://www.elastic.co/guide/index.html" target="_blank" rel="noreferrer">https://www.elastic.co/guide/index.html</a></p></blockquote><h2 id="es语法" tabindex="-1">ES语法 <a class="header-anchor" href="#es语法" aria-label="Permalink to &quot;ES语法&quot;">​</a></h2><table tabindex="0"><thead><tr><th>概念</th><th>说明</th></tr></thead><tbody><tr><td>索引库（indices)</td><td>indices是index的复数，代表索引集</td></tr><tr><td>文档（document）</td><td>存入索引库原始的数据。比如每一条商品信息，就是一个文档</td></tr><tr><td>字段（field）</td><td>文档中的属性</td></tr><tr><td>映射配置（mappings）</td><td>字段的数据类型、属性、是否索引、是否存储等特性</td></tr></tbody></table><h3 id="查询" tabindex="-1">查询 <a class="header-anchor" href="#查询" aria-label="Permalink to &quot;查询&quot;">​</a></h3><ul><li>基本查询</li><li>结果过滤_source</li><li>高级查询</li><li>filter过滤</li><li>排序</li></ul><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GET _search</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;query&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;match_all&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><blockquote><p>返回结果分析</p><ul><li>took：本次操作花费的时间，单位为毫秒</li><li>time_out：请求是否超时</li><li>_shards：说明本次操作共搜索了哪些分片</li><li>hits：搜索命中的记录</li><li>hits.total：符合条件的文档总数hits.hits：匹配度较高的前N个文档</li><li>hits.max_score：文档匹配得分，这里为最高分</li><li>_score：每个文档都有一个匹配得分，按照降序排列</li><li>source：显示了文档的原始内容</li></ul></blockquote><h2 id="es写数据、读数据过程" tabindex="-1">ES写数据、读数据过程 <a class="header-anchor" href="#es写数据、读数据过程" aria-label="Permalink to &quot;ES写数据、读数据过程&quot;">​</a></h2><p><strong>es写数据过程</strong>：</p><ul><li>客户端选择一个node发送请求过去，这个node就是coordinating node协调节点。</li><li>协调节点对document进行路由，将请求转发给对应的node（有primary shard）。</li><li>实际的node上的primary shard处理请求，然后将数据同步到replica node。</li><li>协调节点如果发现primary node和所有的replica node都搞定之后，就返回响应结果给客户端。</li></ul><p><strong>es读数据过程</strong>：通过doc id来查询，会根据doc id进行hash，判断分配到哪个shard。</p><ul><li>客户端发送请求到任意一个node，成为coordinating node协调节点。</li><li>协调节点对doc id进行哈希路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡。</li><li>接收请求的node返回document给coordinatingnode协调节点。</li><li>协调节点返回document给客户端。</li></ul><p><strong>es搜索数据</strong>：最强大的是做全文检索。</p><ul><li>客户端发送请求到一个coordinating node协调节点。</li><li>协调节点将搜索请求转发到所有的shard对应的primary node或replica node。</li><li>query phase：每个shard将自己的搜索结果（其实就是一些doc id）返回给协调节点，由协调节点继续数据的合并、排序、分页等操作，产出最终结果。</li><li>fetch phase：接着由协调节点根据doc id去各个节点上拉取实际的document数据，最终返回给客户端</li></ul><p><strong>总结一下写数据</strong>，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。</p><p><strong>删除/更新数据底层原理</strong>：删除操作，commit的时候会生成一个.del文件，将某个doc标识为deleted状态，搜索的时候根据.del文件就知道这个doc是否被删除了。更新是将原来的doc标识为deleted状态，然后新写入一条数据。segment file默认一秒钟一个，越来越多，定期merge，这是deleted状态doc物理删除。</p><p><strong>数据量大之后查询慢</strong>：单纯去查segment file磁盘文件，速度得按秒级来，但是es有机制是将查询数据缓存到filesystem cache，理论上filesystem cache够大缓存所有，那么速度可以毫秒级。最佳是至少一半数据放到filesystem cache，当然也可以把索引数据都放到filesystem cache，例如一行数据三十个字段，只cache其中三个要查的数据，采用es+mysql/hbase，一般建议es+hbase。如果还是多，那就数据预热、冷热分离（冷热数据各写入一个索引，不同机器以使尽可能保证cache热数据）。</p><h2 id="references" tabindex="-1">References <a class="header-anchor" href="#references" aria-label="Permalink to &quot;References&quot;">​</a></h2><ul><li><a href="https://juejin.cn/post/7070469367456071717#heading-19" target="_blank" rel="noreferrer">Elasticsearch业务常用21种语句操作附Java Api代码</a></li><li><a href="https://blog.csdn.net/kavito/article/details/88290222?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-14.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-14.pc_relevant_paycolumn_v3&amp;utm_relevant_index=18" target="_blank" rel="noreferrer">Elasticsearch的简单入门：（二）ES基础</a></li><li><a href="https://juejin.cn/post/7294638699417206794" target="_blank" rel="noreferrer">巧记Elasticsearch常用DSL语法</a></li><li><a href="https://juejin.cn/post/7223586933881061413#heading-0" target="_blank" rel="noreferrer">10个实用的Elasticsearch查询技巧</a></li><li><a href="https://juejin.cn/post/6844903664717398023#heading-7" target="_blank" rel="noreferrer">23个最有用的Elasticsearch检索技巧</a></li><li><a href="https://juejin.cn/post/7137679778944909319#heading-29" target="_blank" rel="noreferrer">一文彻底搞懂Elasticsearch中Geo数据类型查询、聚合、排序</a></li><li><a href="https://juejin.cn/post/7230703361179549756#heading-85" target="_blank" rel="noreferrer">ElasticSearch分词器详解，一文get</a></li></ul>`,21),r=[l];function n(o,c,d,h,p,u){return t(),a("div",null,r)}const f=e(s,[["render",n]]);export{m as __pageData,f as default};
